---
title: "QFWUnit6CMallavarapu_CAlvarado_SKC"
output:
  html_document: default
  word_document: default
---
# Authors: Chiranjeevi Mallavarapu, Cynthia Alvarado, Sabitri KC
## 1 Abstract
In this case study, we examine Real Time Location systems (RTLS) to predict a location using indoor positioning systems based on retrieving the media access control (MAC) address of a device.  We use k-nearest neighbors (KNN) to determine locations of devices within a defined floorplan. We examine the potential issues regarding the use or non-use of certain MAC addresses and investigate the effect on accuracy of an indoor positioning system developed by Nolan and Temple Lang. We also hoped to try another possible enhancement approach using a weighted k-nearest neighbors for determining location but due to limited time left we could not proceed further. But based on the initial data exploration, as signal stregths seemed to have strong relation to distance, we expect that weighted KNN gives a better result.

## 2 Introduction
Real Time Location Systems have a wide range of practical applications. These RTLS applications are called indoor positioning systems.  In particular, indoor positioning can help with identifying personnel within the building, ensuring the safety of those individuals in emergency situations, for example.  It may also deter theft by identifying valuable assets, tracking the position of the asset within the building or being notified if it is outside the confines of the building. Tracing the previous location, identifying the current location and predicting the future locations of a package is a popular use of RTLS technology. Proliferation of tracking tags with the advancement in wireless technologies have made real-time location systems ubiquitous in every sector like inventory management, livestock maintenance, personnel tracking, healthcare manufacturing and navigation.

In this paper, we will be doing a more thorough data analysis using the data that is available for the Nolan and Lang textbook. The data were collected from a single mobile device at 166 positions on the floor plan of the building. Eight orientation angles were considered at each location and 110 readings were taken for each (x, y) position, angle combination. For each reading, 6 access points provide signal strength data to the mobile device as orginally considered by the authors. Nolan and Lang provide an indoor positioning system using signal strength data and a mean-based k-nearest neighbors approach using six access points and drop one access point which was originally in the data. 

In addition to reviewing above author's approach in the start, we will also assess 2 different models one with utilizing the access point which was previously dropped on author’s model and another with including the new MAC and excluding the one considered in authors model. We also implement a weighted k-nearest neighbors model for predicting the location of previously unseen signal strength data rather than their mean-based k-nearest neighbors approach and improve the analysis.

## 3 Background
 The data used in this study consists of an “offline” training set and an “online” test set as given by the textbook. The offline data contains measurements of signal strength from a grid of specified measurement points that are one meter apart from each other, as indicated by grey colored dot in the text book. The "online" data set consists of similar readings of subset of the locations from above as indicate in bold black spots in the textbook, which also gives us the physical location of the same. 
 
 There are 6 wireless access points and the signal for each access point is measured from each of these points at 8 different rotational angles that are 45 degrees apart. 
It contains X attributes pertaining to the MAC address. This includes the timestamp of the signal reading, the MAC of the device, the real position, the degree of the orientation of the signal captured, and the signal strength values.  The table of the variable names and a description follows:

```{r echo = FALSE, results = 'asis'}
Variable <- c('t','id','pos','degree','MAC')
Description <- c('Timestamp in milliseconds','MAC address of the scanning device','The physical coordinate of the scanning device','Orientation of the scanning device in degrees','MAC address of the responding access point with corresponding signal strength in decibel-milliwatts (dBm), channel frequency and mode (adhoc is 1 and access point is 3)')
Type <- c('String','String','Continuous','Continuous','String')

VariablesTbl <- data.frame(Variable, Description, Type)
library(knitr)
kable(VariablesTbl[1:5, ], caption = "Table 1: Variable Description and Types
")
```


## 4 Methods

Before discussing about the methods of analysis, we decided to clean, transform and explore our raw data and prepare it for analysis. Our given raw data was provided in 2 separate datasets as offline and online which we will use as training and test data sets. We will be mainly focusing on training dataset for analysis purpose. The training dataset consists of 151,392 lines but according to data collection process we were expecting 146,080 lines in the files (166 locations*8 angles*110recordings).so the differences between two which is 5,312 is the number of comment lines. We extract the data from input file and manipulate it to convert into a structure that is easier to analyze since the original structure contains a non-uniform number of readings for each line. A new dataset for analysis was created by dropping the extra access points, normalizing the orientation values and translating the time variables as in format shown in Table 1.

### 4.1 Data Cleaning and Exploration

Before we are able to analyze this signal data , we must first clean and explore the data.  We will start byreading the data file into a variable named txt.  

```{r}
txt = readLines("http://rdatasciencecases.org/Data/offline.final.trace.txt")
```

Next, we check number of lines in file that begin with a '#' character  
```{r}
sum(substr(txt, 1, 1) == "#")
```

We can determine the number of lines in the file using the length command  
```{r}
length(txt)
```

We expect there to be 146,080 measurements in the file.  Subtracting 5312 (the number of lines starting with a '#' character - i.e. comments) from 151,392 (the number of total lines in the file) gives us 146,080 non-comment lines.  This matches the number of expected measurements exactly and is a good sign that we are not missing any data.  

Next, we will need to convert the data into a structure that is easier to analyze.  The original structure contains a non-uniform number of readings for each line.  We will reshape the data to have one measurement per line.  

First we will see what a line of data looks like if we split it using a semi-colon delimiter  
```{r}
strsplit(txt[4], ";")[[1]]
```

We can see that the resulting strings can be split further using an equals sign delimiter. The values for some of these fields (notably the pos field and the measurements for the responding devices) contain three fields separated by commas.  Further splitting on these delimiters would look like the following output.  

```{r}
unlist(lapply(strsplit(txt[4], ";")[[1]],
              function(x)
                sapply(strsplit(x, "=")[[1]], strsplit, ",")))
```

A more elegant way to do this is to use the strsplit functions ability to process regular expressions and split on all three delimiters at once.  We will do this and save the output in the tokens variable.

```{r}
tokens = strsplit(txt[4], "[;=,]")[[1]]
tokens[1:10]
```

We can extract the first four variables (timestamp, MAC address, position (x, y, and z), and orientation) as follows.
```{r}
tokens[c(2, 4, 6:8, 10)]
```

We can then look at the readings for all of the responding devices by stripping out the first 10 elements of the array.
```{r}
tokens[ - (1:10)]
```

In order to build out the additional rows needed in order for our data to only include one location per line, we will need to bind each of these sets of 4 values from the responding devices with the initial 4 values in the line.
```{r}
tmp = matrix(tokens[ - (1:10) ], ncol = 4, byrow = TRUE)
mat = cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp),
                   ncol = 6, byrow = TRUE),
            tmp)
```

This should produce a matrix containing 11 columns that each have 10 rows.  We can verify that with the dim command.
```{r}
dim(mat)
```

Now that we've determined how to reshape the data, we will create a function to do this so that it can be run on every line in the input file.  This function will return NULL for any lines that contain no responding device measurements, as this data provides no value to this analysis.
```{r}
processLine = 
  function(x) 
  { 
    tokens = strsplit(x, "[;=,]")[[1]]
    
    if (length(tokens) == 10)
      return(NULL)
    
    tmp = matrix(tokens[ - (1:10)], ncol = 4, byrow = TRUE) 
    cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp),
                 ncol = 6, byrow = TRUE), tmp) }
```

We can test this function on the first few lines of the file to make sure it works properly.  The first 3 lines in the file are comments, so those will be skipped.  We will start on row 4 and finish with line 20.  This should produce 17 matrices with varying numbers of rows depending on how many responding device signals they contain.
```{r}
tmp = lapply(txt[4:20], processLine)
sapply(tmp, nrow)
```

The last step is to combine all of these individual matrices into one dataframe.  This can be done efficiently using the do.call() method.
```{r}
offline = as.data.frame(do.call("rbind", tmp))
dim(offline)
```
This results in a dataframe containing 10 columns and 170 rows, which is the expected result.

Now that the data refactoring steps have been tested successfully on a small subset of the data file, they can be run on the full dataset.  The lines containing comments will be ignored by skipping lines tht begin with a '#' character.

```{r}
lines = txt[ substr(txt, 1, 1) != "#" ]
tmp = lapply(lines, processLine)
offline = as.data.frame(do.call("rbind", tmp),
                        stringsAsFactors = FALSE)
dim(offline)
```
The resulting dataset stored in the offline variable contains 1,181,628 rows with one measurement each.

In order to make the dataset easier to interpret, column names can be added.
```{r}
names(offline) = c("time", "scanMac", "posX", "posY", "posZ", "orientation", "mac", "signal", "channel", "type")
```

The position, signal, and time variables can be converted from strings to numeric data types.
```{r}
numVars = c("time", "posX", "posY", "posZ",
            "orientation", "signal")
offline[ numVars ] = lapply(offline[ numVars ], as.numeric)
```

The type variable is set to 1 for adhoc devices and 3 for access points.  This analysis will only include access points.  Therefore, any rows where type is not equal to 3 can be dropped.  The type column can also be dropped, since it will now contain the same value for every record.
```{r}
offline = offline[ offline$type == "3", ]
offline = offline[ , "type" != names(offline) ]
dim(offline)
```
The resulting dataset now contains 978,443 records and only 9 columns instead of the original 10.

Next, we will convert the time column to a datetime format.  This column currently contains the number of milliseconds from the UNIX epoch.  Since the POSIXt format uses the number of seconds from the epoch instead of milliseconds, the data will need to be scaled.  We will also keep the original time format in a new column named rawTime.
```{r}
offline$rawTime = offline$time
offline$time = offline$time/1000
class(offline$time) = c("POSIXt", "POSIXct")
```

To verify that all of these operations were completed successfully and the data is in the desired format, we can check the column names and data types.
```{r}
unlist(lapply(offline, class))
```

Now that the format of the dataframe is correct, we also need to verify that the format of the data itself is in the correct format.  First, the summary statistics for the data are examined.

```{r}
summary(offline[, numVars])
```

Since some of the variables are categorical, converting these variables to factors before running the summary command will reveal more information.
```{r}
summary(sapply(offline[ , c("mac", "channel", "scanMac")],
               as.factor))
```
We can see from this output that the scanMac column contains the same value (00:02:2D:21:0F:33:978443) for every row.  This is because the hand-held device that was used to take the measurements was the same for every measurement.  We can also see the the posZ (elevation of the hand-held device) is 0 for every row.  We can safely drop these columns, since they add no value to the analysis.

```{r}
offline = offline[ , !(names(offline) %in% c("scanMac", "posZ"))]
```

The documentation for the data states that there should be 8 values starting at 0 and separated by 45 degrees (0, 45, 90, 135, 180, 225, 270, 315).  This can be verified by counting the number of unique values in the orientation field.
```{r}
length(unique(offline$orientation))
```

There are 203 unique values in the orientation field, so we will need to explore this variable further to determine where these extra values are coming from.  First, we can plot the values to examine the distribution.

```{r}
plot(ecdf(offline$orientation))
```
We can see from the plot that the orientations are all clustered around the 8 expected values.  This is most likely due to the hand-held device not being oriented in precisely the desired direction.  Since we would like to analyze the 8 defined orientations and the actual values are close, we can convert the actual values to their closest desired value.  360 degrees is equivalent to 0 degrees, so we will convert values near 360 to 0 degrees and not 315 (which is technically the closest desired value).  The original values will be saved and the rounded values will be stored in a new variable called angle.
```{r}
roundOrientation = function(angles) {
  refs = seq(0, by = 45, length = 9)
  q = sapply(angles, function(o) which.min(abs(o - refs)))
  c(refs[1:8], 0)[q]
}

offline$angle = roundOrientation(offline$orientation)
```

The new angles can be validated with a box plot.
```{r}
with(offline, boxplot(orientation ~ angle,
                      xlab = "nearest 45 degree angle",
                      ylab = "orientation"))
```
The function appears to have worked as expected, including setting orientations near 360 degrees to an angle of 0.

When running the summary command earlier, it appeared that the number of occurences of some MAC addresses may have matched up with the number of occurences of some channels.  More investigation is needed in order to determine if there is a one to one relationship between MAC addresses and channels.  If that is the case, we will be able to remove the channel column.  First, we will count the number of unique MAC addresses and unique channels.
```{r}
c(length(unique(offline$mac)), length(unique(offline$channel)))
```
We can see from the output that there are 12 MAC addresses instead of the expected 6.  The additional 2 MAC addresses are not on the floor plan and therefore not part of the testing.  The MAC addresses can be investigated further using the table command.
```{r}
table(offline$mac)
```
The first, eleventh, and twelfth MAC addresses only have a handful of recordings and are most likely not part of the study.  The third and fifth MAC addresses also have significantly fewer measurements than the rest of the devices.  

The documentation specifies that 5 of the devices are made by Linksys/Cisco and the sixth is a Lancom L-54g router.  The first three hex pairs in a MAC address specify the manufacturer of the device.  Knowing this, we can determine that the 5 MAC addresses starting with 00:14:bf are Linksys/Cisco devices.  Lancom MAC addresses begin with 00:a0:57, but the dataset does not contain any devices that match that pattern.  For now, we will keep the 7 devices with the highest number of measurements.
```{r}
subMacs = names(sort(table(offline$mac), decreasing = TRUE))[1:7]
offline = offline[ offline$mac %in% subMacs, ]
```

Now we will check the number of channels associated with the remaining MAC addresses to see if there is a one to one relationship.
```{r}
macChannel = with(offline, table(mac, channel))
apply(macChannel, 1, function(x) sum(x > 0))
```
Our suspicion that there is a one to one relationship between MAC addresses and channels is validated.  We can remove the channel column, as it does not add any additional information to the analysis.
```{r}
offline = offline[ , "channel" != names(offline)]
```

Next, the columns containing x and y positions for the hand-held device need to be explored.  First, we will create a list dataframes that contains each location.
```{r}
locDF = with(offline,
             by(offline, list(posX, posY), function(x) x))
length(locDF)
```
This list contains 476 dataframes, which is more than the number of locations where measurements were recorded.  We will check to see if any of these locations are null.
```{r}
sum(sapply(locDF, is.null))
```
There are 310 null locations contained in the list of dataframes.  We can drop these values.
```{r}
locDF = locDF[ !sapply(locDF, is.null) ]
length(locDF)
```
We now have 166 locations, which is consistent with the number of locations where observations were made.

### 4.2 Further Data Exploration

Next we can create a matrix conatining the count of observations for each location.
```{r}
locCounts = sapply(locDF,
                   function(df)
                     c(df[1, c("posX", "posY")], count = nrow(df)))
class(locCounts)
dim(locCounts)
locCounts[ , 1:8]
```
The locCounts variable is a 3 row matrix with 166 columns.  Each column corresponds to a location.  The rows detail the x position, the y position and the count of observations at each position.  

By plotting these counts using their x and y locations, we can more easily visualize where on the floor plan the measurements were taken.
```{r}
locCounts = t(locCounts)
plot(locCounts, type = "n", xlab = "", ylab = "")
text(locCounts, labels = locCounts[,3], cex = .8, srt = 45)
```
We will create a function called readData() that contains all of this data cleaning code in order to easily re-run it later.
```{r}
readData = 
  function(filename = 'http://rdatasciencecases.org/Data/offline.final.trace.txt', 
           subMacs = c("00:0f:a3:39:e1:c0", "00:0f:a3:39:dd:cd", "00:14:bf:b1:97:8a",
                       "00:14:bf:3b:c7:c6", "00:14:bf:b1:97:90", "00:14:bf:b1:97:8d",
                       "00:14:bf:b1:97:81"))
  {
    txt = readLines(filename)
    lines = txt[ substr(txt, 1, 1) != "#" ]
    tmp = lapply(lines, processLine)
    offline = as.data.frame(do.call("rbind", tmp), 
                            stringsAsFactors= FALSE) 
    
    names(offline) = c("time", "scanMac", 
                       "posX", "posY", "posZ", "orientation", 
                       "mac", "signal", "channel", "type")
    
     # keep only signals from access points
    offline = offline[ offline$type == "3", ]
    
    # drop scanMac, posZ, channel, and type - no info in them
    dropVars = c("scanMac", "posZ", "channel", "type")
    offline = offline[ , !( names(offline) %in% dropVars ) ]
    
    # drop more unwanted access points
    offline = offline[ offline$mac %in% subMacs, ]
    
    # convert numeric values
    numVars = c("time", "posX", "posY", "orientation", "signal")
    offline[ numVars ] = lapply(offline[ numVars ], as.numeric)

    # convert time to POSIX
    offline$rawTime = offline$time
    offline$time = offline$time/1000
    class(offline$time) = c("POSIXt", "POSIXct")
    
    # round orientations to nearest 45
    offline$angle = roundOrientation(offline$orientation)
      
    return(offline)
  }
```

Next, we will examine how the signal changes for a given location based on orientation of the scanning device.
```{r}
library(lattice)
bwplot(signal ~ factor(angle) | mac, data = offline,
       subset = posX == 2 & posY == 12 
       & mac != "00:0f:a3:39:dd:cd",
       layout = c(2,3))
```
We can see from these plots that the mean signal strength and variance differ fairly widely for a specific location given different orientations and responding devices.

The summary statistics for the signal variable show that the values are all negative.  Weaker signals have a higher absolute value and stronger signals have a lower absolute value
```{r}
summary(offline$signal)
```

```{r}
densityplot( ~ signal | mac + factor(angle), data = offline,
             subset = posX == 24 & posY == 4 &
               mac != "00:0f:a3:39:dd:cd",
             bw = 0.5, plot.points = FALSE)
```

Next, we will take a closer look at summary statistics broken down by location.  Given that there are so many data points, this will necessitate some data manipulation.  First, we will create a new variable that contains all combinations of the x and y coordinates of the scanning device.
```{r}
offline = offline[ offline$mac != "00:0f:a3:39:dd:cd", ]
offline$posXY = paste(offline$posX, offline$posY, sep = "-")
```

We will now create another variable that contains every combination of posXY, angle, and access point MAC address.
```{r}
byLocAngleAP = with(offline,
                    by(offline, list(posXY, angle, mac),
                       function(x) x))
```

We can now calculate the summary statistics for each of these data frames.

```{r}
signalSummary = 
  lapply(byLocAngleAP, 
         function(oneLoc) { 
           ans = oneLoc[1,] 
           ans$medSignal = median(oneLoc$signal) 
           ans$avgSignal = mean(oneLoc$signal) 
           ans$num = length(oneLoc$signal)
           ans$sdSignal = sd(oneLoc$signal)
           ans$iqrSignal = IQR(oneLoc$signal)
           ans
         })

offlineSummary = do.call("rbind", signalSummary)
```

First we will see if the standard deviation of signal strength varies with mean signal strength for each location/angle/access point combination.
```{r}
breaks = seq(-90, -30, by = 5)
bwplot(sdSignal ~ cut(avgSignal, breaks = breaks),
       data = offlineSummary,
       subset = mac != "00:0f:a3:39:dd:cd",
       xlab = "Mean Signal", ylab = "SD Signal")
```
We can see in this box and whisker plot that standard deviation appears to increase with mean signal strength.

We can also see if signal strength is skewed by plotting the difference between average and median signal strength.
```{r}
with(offlineSummary,
     smoothScatter((avgSignal - medSignal) ~ num,
                   xlab = "Number of Observations",
                   ylab = "mean - median"))
abline(h = 0, col = "#984ea3", lwd = 2)

lo.obj = 
  with(offlineSummary, 
       loess(diff ~ num, 
             data = data.frame(diff = (avgSignal - medSignal), 
                               num = num)))

lo.obj.pr = predict(lo.obj, newdata = data.frame(num = (70:120)))
lines(x = 70:120, y = lo.obj.pr, col = "#4daf4a", lwd = 2)
```
The mean and median do not appear to be significantly different from each other, which means the data does not have a substantial skew.

### 4.3 Methodology for analysis and prediction using k-Nearest Neighbors 

We will now use k-Nearest Neighbors (k-NN) to determine location from access point signal strength.  k-NN works by measuring the distance between a new point and it's neighboring points.  The k closest neighboring points to the newest point are used to vote on the value for the new point.  For example, let's say k is equal to 5.  If 3 of the 5 closest points are red and 2 of the 5 closest points are blue, then we assign a value of red to the new point.  

First, we will prepare the data to run k-NN.
```{r}
macs = unique(offlineSummary$mac)
online = readData("http://rdatasciencecases.org/Data/online.final.trace.txt", subMacs = macs)
```

With this new dataset, we will again create unique location identifiers and verify that there are 60 of them.
```{r}
online$posXY = paste(online$posX, online$posY, sep = "-")
length(unique(online$posXY))
```

We will also count the number of signal strengths recorded at each test location.
```{r}
tabonlineXYA = table(online$posXY, online$angle)
tabonlineXYA[1:6, ]
```
It appears that signal strengths were only recorded at one angle for each test location in this data.  To account for this, we will reorganize the data to contain a column for each access point containing the average signal strength.
```{r}
keepVars = c("posXY", "posX","posY", "orientation", "angle") 
byLoc = with(online, 
             by(online, list(posXY), 
                function(x) { 
                  ans = x[1, keepVars] 
                  avgSS = tapply(x$signal, x$mac, mean) 
                  y = matrix(avgSS, nrow = 1, ncol = 6, 
                             dimnames = list(ans$posXY, names(avgSS))) 
                  cbind(ans, y) 
                  }))

onlineSummary = do.call("rbind", byLoc)

dim(onlineSummary)
names(onlineSummary)
```

Next, we need to find the orientations to include from the training dataset.
```{r}
m = 3; angleNewObs = 230
refs = seq(0, by = 45, length = 8)
nearestAngle = roundOrientation(angleNewObs)

if (m %% 2 == 1) {
  angles = seq(-45 * (m - 1) / 2, 45 * (m - 1) / 2, length = m)
} else {
  m = m + 1
  angles = seq(-45 * (m - 1) / 2, 45 * (m - 1) / 2, length = m)
  if (sign(angleNewObs - nearestAngle) > -1)
    angles = angles[ -1 ]
  else
    angles = angles[ -m ]
}

angles = angles + nearestAngle
angles[angles < 0] = angles[ angles < 0 ] + 360
angles[angles > 360] = angles[ angles > 360 ] - 360
```

Now that we have determined the angles to use, we will select the rows containing these values from the training set.
```{r}
offlineSubset = offlineSummary[ offlineSummary$angle %in% angles, ]
```

We will now aggregate the signal strengths from these angles in order to reshape them to the same form as onlineSummary.  In order to do this, we will create a helper function called reshapeSS and wrap it in another function called selectTrain.
```{r}
reshapeSS = function(data, varSignal = "signal",
                     keepVars = c("posXY", "posX", "posY")) {
  byLocation = 
    with(data, by(data, list(posXY),
                  function(x) {
                    ans = x[1, keepVars]
                    avgSS = tapply(x[ , varSignal ], x$mac, mean)
                    y = matrix(avgSS, nrow = 1, ncol = 6,
                               dimnames = list(ans$posXY,
                                               names(avgSS)))
                    cbind(ans, y)
                  }))
  newDataSS = do.call("rbind", byLocation)
  return(newDataSS)
}

trainSS = reshapeSS(offlineSubset, varSignal = "avgSignal")

selectTrain = function(angleNewObs, signals = NULL, m = 1){
  # m is the number of angles to keep between 1 and 5
  refs = seq(0, by = 45, length  = 8)
  nearestAngle = roundOrientation(angleNewObs)
  
  if (m %% 2 == 1) 
    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
  else {
    m = m + 1
    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
    if (sign(angleNewObs - nearestAngle) > -1) 
      angles = angles[ -1 ]
    else 
      angles = angles[ -m ]
  }
  angles = angles + nearestAngle
  angles[angles < 0] = angles[ angles < 0 ] + 360
  angles[angles > 360] = angles[ angles > 360 ] - 360
  angles = sort(angles) 
  
  offlineSubset = signals[ signals$angle %in% angles, ]
  reshapeSS(offlineSubset, varSignal = "avgSignal")
}
```

This function can be tested using an angle of 130 and an m of 3.  This will aggregate offline data for angles of 90, 135, and 180 degrees.  It will average the signal strength for these three angles.
```{r}
train130 = selectTrain(130, offlineSummary, m = 3)
head(train130)
```

```{r}
length(train130[[1]])
```

Next, we need to create a function to find the nearest neighbor to a given point.  This function will be called findNN.
```{r}
findNN = function(newSignal, trainSubset) { 
  diffs = apply(trainSubset[, 4:9], 1, 
                function(x) x - newSignal) 
  dists = apply(diffs, 2, function(x) sqrt(sum(x^2))) 
  closest = order(dists) 
  return(trainSubset[closest, 1:3]) 
}
```

We can now roll the trainSelect() and findNN() functions into another function that will use kNN to predict location.  We will call that function predXY.
```{r}
predXY = function(newSignals, newAngles, trainData,
                  numAngles = 1, k = 3){
  closeXY = list(length = nrow(newSignals))
  
  for (i in 1:nrow(newSignals)) {
    trainsSS = selectTrain(newAngles[i], trainData, m = numAngles)
    closeXY[[i]] = 
      findNN(newSignal = as.numeric(newSignals[i, ]), trainSS)
  }
  
  estXY = lapply(closeXY,
                 function(x) sapply(x[ ,2:3],
                                    function(x) mean(x[1:k])))
  estXY = do.call("rbind", estXY)
  return(estXY)
}
```

We can test this function using 3 angles anf both 3 nearest neighbors and 1 nearest neighbor.
```{r}
estXYk3 = predXY(newSignals = onlineSummary[ , 6:11],
                 newAngles = onlineSummary[ , 4],
                 offlineSummary, numAngles = 3, k = 3)

estXYk1 = predXY(newSignals = onlineSummary[ , 6:11],
                 newAngles = onlineSummary[ , 4],
                 offlineSummary, numAngles = 3, k = 1)
```

Now we can caluclate the error for each of these test sets and compare them to see if using 1 or 3 neighbors produced more accurate results.
```{r}
calcError =
  function(estXY, actualXY)
    sum(rowSums((estXY - actualXY)^2))

actualXY = onlineSummary[ , c("posX", "posY")]
sapply(list(estXYk1, estXYk3), calcError, actualXY)
```
We can see that using 3 nearest neighbors produces a lower error rate than using 1 nearest neighbor.

In order to determine the optimal value for k, we will use cross validation.
```{r}
v = 11
permuteLocs = sample(unique(offlineSummary$posXY))
permuteLocs = matrix(permuteLocs, ncol = v, 
                     nrow = floor(length(permuteLocs)/v))

onlineFold = subset(offlineSummary, posXY %in% permuteLocs[ , 1])
```

Now the data needs to be summarized and reshaped into a form that matches onlineSummary.  We will need to select a random angle for each location though.  The previously created reshapeSS function can be rewritten to do this.
```{r}
reshapeSS = function(data, varSignal = "signal", 
                     keepVars = c("posXY", "posX","posY"),
                     sampleAngle = FALSE, 
                     refs = seq(0, 315, by = 45)) {
  byLocation =
    with(data, by(data, list(posXY), 
                  function(x) {
                    if (sampleAngle) {
                      x = x[x$angle == sample(refs, size = 1), ]}
                    ans = x[1, keepVars]
                    avgSS = tapply(x[ , varSignal ], x$mac, mean)
                    y = matrix(avgSS, nrow = 1, ncol = 6,
                               dimnames = list(ans$posXY,
                                               names(avgSS)))
                    cbind(ans, y)
                  }))

  newDataSS = do.call("rbind", byLocation)
  return(newDataSS)
}
```

The new version of reshapeSS() can now be used to summarize and format the offline data.
```{r}
offline = offline[ offline$mac != "00:0f:a3:39:dd:cd", ]
keepVars = c("posXY", "posX", "posY", "orientation", "angle")
onlineCVSummary = reshapeSS(offline, keepVars = keepVars,
                            sampleAngle = TRUE)
```

We can now analyze the first online fold and first offline fold to using the previsouly created predXY() function to see if the error rate has improved.
```{r}
onlineFold = subset(onlineCVSummary, 
                    posXY %in% permuteLocs[ , 1])

offlineFold = subset(offlineSummary,
                     posXY %in% permuteLocs[ , -1])

estFold = predXY(newSignals = onlineFold[ , 6:11], 
                 newAngles = onlineFold[ , 4], 
                 offlineFold, numAngles = 3, k = 3)

actualFold = onlineFold[ , c("posX", "posY")]
calcError(estFold, actualFold)
```
The error of 83 using cross validation is much lower than the original error rate of 482.

Now we will step through different values for k to find the optimal number of nearest neighbors.  We will test values between 1 and 20 and calculate the error rate for each of them.
```{r}
K = 20
err = rep(0, K)

for (j in 1:v) {
  onlineFold = subset(onlineCVSummary, 
                      posXY %in% permuteLocs[ , j])
  offlineFold = subset(offlineSummary,
                       posXY %in% permuteLocs[ , -j])
  actualFold = onlineFold[ , c("posX", "posY")]
  
  for (k in 1:K) {
    estFold = predXY(newSignals = onlineFold[ , 6:11],
                     newAngles = onlineFold[ , 4], 
                     offlineFold, numAngles = 3, k = k)
    err[k] = err[k] + calcError(estFold, actualFold)
  }
}
```

```{r}
plot(y = err, x = (1:K),  type = "l", lwd= 2,
     ylim = c(600, 2100),
     xlab = "Number of Neighbors",
     ylab = "Sum of Square Errors")

rmseMin = min(err)
kMin = which(err == rmseMin)[1]
segments(x0 = 0, x1 = kMin, y0 = rmseMin, col = gray(0.4), 
         lty = 2, lwd = 2)
segments(x0 = kMin, x1 = kMin, y0 = 1100,  y1 = rmseMin, 
         col = grey(0.4), lty = 2, lwd = 2)

#mtext(kMin, side = 1, line = 1, at = kMin, col = grey(0.4))
text(x = kMin - 2, y = rmseMin + 40, 
     label = as.character(round(rmseMin)), col = grey(0.4))
```
Looking at the plot of sum of squared error as a function of k, 5 appears to be the optimal value of k with a sum of squared error of 1341.

We can now generate location predictions using a k of 5 and calculate the number of errors.
```{r}
estXYk5 = predXY(newSignals = onlineSummary[ , 6:11],
                 newAngles = onlineSummary[ , 4],
                 offlineSummary, numAngles = 3, k =5)

calcError(estXYk5, actualXY)
```
The error rate of 454 for the estimation created using 5 nearest neighbors is lower than the estimations that used both 1 and 3 nearest neighbors (579 and 483 respectively).  

### 4.4 Omitted MAC address: a model with Including it, a model with replacing it

When cleaning this data, the access points with MAC addresses of 00:0f:a3:39:e1:c0 and 00:0f:a3:39:dd:cd looked anomalous.  We decided to keep the first and remove the second.  We will now rerun the kNN analysis using both of these MAC addresses as well as using the second address without the first.  Since this MAC address has been removed from the original offline dataset, this data needs to be reconstructed.
```{r}
offlineRedo = readData()

offlineRedo$posXY = paste(offlineRedo$posX, offlineRedo$posY, sep = "-")

byLocAngleAP = with(offlineRedo, 
                    by(offlineRedo, list(posXY, angle, mac), 
                       function(x) x))

signalSummary = 
  lapply(byLocAngleAP,            
         function(oneLoc) {
           ans = oneLoc[1, ]
           ans$medSignal = median(oneLoc$signal)
           ans$avgSignal = mean(oneLoc$signal)
           ans$num = length(oneLoc$signal)
           ans$sdSignal = sd(oneLoc$signal)
           ans$iqrSignal = IQR(oneLoc$signal)
           ans
           })

offlineSummaryRedo = do.call("rbind", signalSummary)
```

The online dataset will also need to be recreated with all 7 MAC addresses.
```{r}
macs = unique(offlineSummaryRedo$mac)
onlineRedo = readData("http://rdatasciencecases.org/Data/online.final.trace.txt", subMacs = macs)

onlineRedo$posXY = paste(onlineRedo$posX, onlineRedo$posY, sep = "-")

keepVars = c("posXY", "posX","posY", "orientation", "angle")
byLoc = with(onlineRedo, 
             by(onlineRedo, list(posXY), 
                function(x) {
                  ans = x[1, keepVars]
                  avgSS = tapply(x$signal, x$mac, mean)
                  y = matrix(avgSS, nrow = 1, ncol = 7,
                        dimnames = list(ans$posXY, names(avgSS)))
                  cbind(ans, y)
                }))

onlineSummaryRedo = do.call("rbind", byLoc)

offlineSubsetRedo = offlineSummaryRedo[ offlineSummaryRedo$angle %in% angles, ]

reshapeSS = function(data, varSignal = "signal", 
                     keepVars = c("posXY", "posX","posY"),
                     sampleAngle = FALSE, 
                     refs = seq(0, 315, by = 45)) {
  byLocation =
    with(data, by(data, list(posXY), 
                  function(x) {
                    if (sampleAngle) {
                      x = x[x$angle == sample(refs, size = 1), ]}
                    ans = x[1, keepVars]
                    avgSS = tapply(x[ , varSignal ], x$mac, mean)
                    y = matrix(avgSS, nrow = 1, ncol = 7,
                               dimnames = list(ans$posXY,
                                               names(avgSS)))
                    cbind(ans, y)
                  }))

  newDataSS = do.call("rbind", byLocation)
  return(newDataSS)
}

trainSS = reshapeSS(offlineSubsetRedo, varSignal = "avgSignal")

selectTrain = function(angleNewObs, signals = NULL, m = 1){
  # m is the number of angles to keep between 1 and 5
  refs = seq(0, by = 45, length  = 8)
  nearestAngle = roundOrientation(angleNewObs)
  
  if (m %% 2 == 1) 
    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
  else {
    m = m + 1
    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
    if (sign(angleNewObs - nearestAngle) > -1) 
      angles = angles[ -1 ]
    else 
      angles = angles[ -m ]
  }
  angles = angles + nearestAngle
  angles[angles < 0] = angles[ angles < 0 ] + 360
  angles[angles > 360] = angles[ angles > 360 ] - 360
  angles = sort(angles) 
  
  offlineSubsetRedo = signals[ signals$angle %in% angles, ]
  reshapeSS(offlineSubsetRedo, varSignal = "avgSignal")
}

keepVars = c("posXY", "posX", "posY", "orientation", "angle")
onlineCVSummaryRedo = reshapeSS(offlineRedo, keepVars = keepVars,
                            sampleAngle = TRUE)
```

Now that all of the datasets have been refactored to include both of the anomalous access point MAC addresses, we can determine the optimal number of nearest neighbors to use for the new data.
```{r}
K = 20
err = rep(0, K)

for (j in 1:v) {
  onlineFoldRedo = subset(onlineCVSummaryRedo, 
                      posXY %in% permuteLocs[ , j])
  offlineFoldRedo = subset(offlineSummaryRedo,
                       posXY %in% permuteLocs[ , -j])
  actualFoldRedo = onlineFoldRedo[ , c("posX", "posY")]
  
  for (k in 1:K) {
    estFoldRedo = predXY(newSignals = onlineFoldRedo[ , 6:11],
                     newAngles = onlineFoldRedo[ , 4], 
                     offlineFoldRedo, numAngles = 3, k = k)
    err[k] = err[k] + calcError(estFoldRedo, actualFoldRedo)
  }
}
```

```{r}
plot(y = err, x = (1:K),  type = "l", lwd= 2,
     ylim = c(1200, 2100),
     xlab = "Number of Neighbors",
     ylab = "Sum of Square Errors")

rmseMin = min(err)
kMin = which(err == rmseMin)[1]
segments(x0 = 0, x1 = kMin, y0 = rmseMin, col = gray(0.4), 
         lty = 2, lwd = 2)
segments(x0 = kMin, x1 = kMin, y0 = 1100,  y1 = rmseMin, 
         col = grey(0.4), lty = 2, lwd = 2)

#mtext(kMin, side = 1, line = 1, at = kMin, col = grey(0.4))
text(x = kMin - 2, y = rmseMin + 40, 
     label = as.character(round(rmseMin)), col = grey(0.4))
```
We can see that the optimal number of nearest neighbors to use is now 6 instead of 5.  The sum of square error is 1269.  We will run kNN on the data and compare the calculated error with that of the original kNN run on the dataset that did not include one of the MAC addresses.

```{r}
actualXYRedo = onlineSummaryRedo[ , c("posX", "posY")]

estXYk6 = predXY(newSignals = onlineSummaryRedo[ , 6:11],
                 newAngles = onlineSummaryRedo[ , 4],
                 offlineSummaryRedo, numAngles = 3, k = 6)

calcError(estXYk6, actualXYRedo)
```
### 4.5 Conclusion : Including the MAC address in question 
When using all 7 access point MAC addresses, we get an error rate of 448, which is slightly better than the error rate (454) for the analysis that not include 00:0f:a3:39:dd:cd.  Next, we will run the analysis again using 00:0f:a3:39:dd:cd, but removing the access point with the MAC address of 00:0f:a3:39:e1:c0.  We will again need to rebuild the data set.

```{r}
offlineRedo2 = offlineRedo[ offlineRedo$mac != "00:0f:a3:39:e1:c0", ]


offlineRedo2$posXY = paste(offlineRedo2$posX, offlineRedo2$posY, sep = "-")

byLocAngleAP = with(offlineRedo2, 
                    by(offlineRedo2, list(posXY, angle, mac), 
                       function(x) x))

signalSummary = 
  lapply(byLocAngleAP,            
         function(oneLoc) {
           ans = oneLoc[1, ]
           ans$medSignal = median(oneLoc$signal)
           ans$avgSignal = mean(oneLoc$signal)
           ans$num = length(oneLoc$signal)
           ans$sdSignal = sd(oneLoc$signal)
           ans$iqrSignal = IQR(oneLoc$signal)
           ans
           })

offlineSummaryRedo2 = do.call("rbind", signalSummary)
```

The online dataset will also need to be rebuilt
```{r}
macs = unique(offlineSummaryRedo2$mac)
onlineRedo2 = readData("http://rdatasciencecases.org/Data/online.final.trace.txt", subMacs = macs)

onlineRedo2$posXY = paste(onlineRedo2$posX, onlineRedo2$posY, sep = "-")

keepVars = c("posXY", "posX","posY", "orientation", "angle")
byLoc = with(onlineRedo2, 
             by(onlineRedo2, list(posXY), 
                function(x) {
                  ans = x[1, keepVars]
                  avgSS = tapply(x$signal, x$mac, mean)
                  y = matrix(avgSS, nrow = 1, ncol = 6,
                        dimnames = list(ans$posXY, names(avgSS)))
                  cbind(ans, y)
                }))

onlineSummaryRedo2 = do.call("rbind", byLoc)

offlineSubsetRedo2 = offlineSummaryRedo2[ offlineSummaryRedo2$angle %in% angles, ]

reshapeSS = function(data, varSignal = "signal", 
                     keepVars = c("posXY", "posX","posY"),
                     sampleAngle = FALSE, 
                     refs = seq(0, 315, by = 45)) {
  byLocation =
    with(data, by(data, list(posXY), 
                  function(x) {
                    if (sampleAngle) {
                      x = x[x$angle == sample(refs, size = 1), ]}
                    ans = x[1, keepVars]
                    avgSS = tapply(x[ , varSignal ], x$mac, mean)
                    y = matrix(avgSS, nrow = 1, ncol = 6,
                               dimnames = list(ans$posXY,
                                               names(avgSS)))
                    cbind(ans, y)
                  }))

  newDataSS = do.call("rbind", byLocation)
  return(newDataSS)
}

trainSS = reshapeSS(offlineSubsetRedo2, varSignal = "avgSignal")

selectTrain = function(angleNewObs, signals = NULL, m = 1){
  # m is the number of angles to keep between 1 and 5
  refs = seq(0, by = 45, length  = 8)
  nearestAngle = roundOrientation(angleNewObs)
  
  if (m %% 2 == 1) 
    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
  else {
    m = m + 1
    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
    if (sign(angleNewObs - nearestAngle) > -1) 
      angles = angles[ -1 ]
    else 
      angles = angles[ -m ]
  }
  angles = angles + nearestAngle
  angles[angles < 0] = angles[ angles < 0 ] + 360
  angles[angles > 360] = angles[ angles > 360 ] - 360
  angles = sort(angles) 
  
  offlineSubsetRedo2 = signals[ signals$angle %in% angles, ]
  reshapeSS(offlineSubsetRedo2, varSignal = "avgSignal")
}

keepVars = c("posXY", "posX", "posY", "orientation", "angle")
onlineCVSummaryRedo2 = reshapeSS(offlineRedo2, keepVars = keepVars,
                            sampleAngle = TRUE)
```

Now that the new datasets have been constructed, the kNN can be run for this third dataset that includes the access point with a MAC address of 00:0f:a3:39:dd:cd, but not the access point with a MAC address of 00:0f:a3:39:e1:c0.  The first step once again is to determine the optimal number of nearest neighbors to use.
```{r}
K = 20
err = rep(0, K)

for (j in 1:v) {
  onlineFoldRedo2 = subset(onlineCVSummaryRedo2, 
                      posXY %in% permuteLocs[ , j])
  offlineFoldRedo2 = subset(offlineSummaryRedo2,
                       posXY %in% permuteLocs[ , -j])
  actualFoldRedo2 = onlineFoldRedo2[ , c("posX", "posY")]
  
  for (k in 1:K) {
    estFoldRedo2 = predXY(newSignals = onlineFoldRedo2[ , 6:11],
                     newAngles = onlineFoldRedo2[ , 4], 
                     offlineFoldRedo2, numAngles = 3, k = k)
    err[k] = err[k] + calcError(estFoldRedo2, actualFoldRedo2)
  }
}
```

```{r}
plot(y = err, x = (1:K),  type = "l", lwd= 2,
     ylim = c(900, 2100),
     xlab = "Number of Neighbors",
     ylab = "Sum of Square Errors")

rmseMin = min(err)
kMin = which(err == rmseMin)[1]
segments(x0 = 0, x1 = kMin, y0 = rmseMin, col = gray(0.4), 
         lty = 2, lwd = 2)
segments(x0 = kMin, x1 = kMin, y0 = 1100,  y1 = rmseMin, 
         col = grey(0.4), lty = 2, lwd = 2)

#mtext(kMin, side = 1, line = 1, at = kMin, col = grey(0.4))
text(x = kMin - 2, y = rmseMin + 40, 
     label = as.character(round(rmseMin)), col = grey(0.4))
```
Using this dataset, the optimal number of nearest neighbors is 2.  We can now run kNN and calculate the error rate of the predicted values compared to the actual values using the calcError function.
```{r}
actualXYRedo2 = onlineSummaryRedo2[ , c("posX", "posY")]

estXYk2 = predXY(newSignals = onlineSummaryRedo2[ , 6:11],
                 newAngles = onlineSummaryRedo2[ , 4],
                 offlineSummaryRedo2, numAngles = 3, k = 2)

calcError(estXYk2, actualXYRedo2)
```
### 4.6 Conclusion : Replacing the MAC address in question in the place of MAC address considered by the Authors

The error rate of 497 using this MAC address is worse than both of the other datasets.  We know that only one of these two devices is an actual access point.  Given that this MAC address used on its own produces a much worse error rate than the other MAC address and using both MAC addresses produces similar results to the dataset that only uses 00:0f:a3:39:e1:c0, we will go forward with the original dataset that includes 00:0f:a3:39:e1:c0 and not 00:0f:a3:39:dd:cd.  

### 4.7 Weighted k-Nearest Neighbors Analysis
Up to this point in this analysis, each nearest neighbor has counted the same as all of the others.  A more accurate might be to assign weights based on how far away the nearest neighbors are from the datapoint being analyzed.  
```{r}
library(dplyr)

wtSignals = function(ByAvgSig){
  signals = ByAvgSig[c(2, 3, 8)] * -1
  locAng = ByAvgSig[c(2, 3, 8)]
  
  weightedSignals = (1/signals) / rowSums(1/signals)
  weightedSignals$posXY = paste(weightedSignals$posX, weightedSignals$posY, sep = "-")
  
  return(cbind(locAng, weightedSignals))
}

wtOfflineByAvgSig = wtSignals(offlineSummary)
wtOnlineByAvgSig = wtSignals(onlineSummary)
```

## 5 Results

Based on our above methods and analysis we first observed that using the textbook approach of calculating the location based on K nearest neighbours including the MAC address, gave us an error rate of 454. As required by the case study we went further to assess if by including the MAC adddress in question improved its accuracy in predicting the location. It seems like the error rate after including the MAC was 402, which is definitely an improvement over the textbook approach. We then, tried by replacing the MAC considered by the authors with this additional MAC and the error rate increased to 497 which is worse than either case. So we conclude that in this scenario, including data from all 7 MAC addresses gives better location predictability. We were hoping to also finish the weighted average analysis for KNN, but due to limited time left, we couldnot finish it.


## References  
1. Lang, D. T., & Nolan, D. A. (2015). Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving. CRC Press. 

2. Ding, Bin, et al. “Application of RTLS in Warehouse Management Based on RFID and Wi-Fi.” IEEE Xplore, 12 Oct. 2008,    ieeexplore.ieee.org/abstract/document/4679157/. 

3. Tarrío, P., et al. “Weighted Least Squares Techniques for Improved Received Signal Strength Based Localization.” Sensors, 2011


